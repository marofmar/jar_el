{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:24:40,839 : INFO : collecting all words and their counts\n",
      "2017-07-20 20:24:40,840 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving the corpus...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:24:46,710 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-07-20 20:24:46,711 : INFO : Loading a fresh vocabulary\n",
      "2017-07-20 20:24:46,929 : INFO : min_count=10 retains 47134 unique words (18% of original 253854, drops 206720)\n",
      "2017-07-20 20:24:46,930 : INFO : min_count=10 leaves 16561031 word corpus (97% of original 17005207, drops 444176)\n",
      "2017-07-20 20:24:47,054 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-07-20 20:24:47,061 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-07-20 20:24:47,062 : INFO : downsampling leaves estimated 12333563 word corpus (74.5% of prior 16561031)\n",
      "2017-07-20 20:24:47,063 : INFO : estimated required memory for 47134 words and 100 dimensions: 61274200 bytes\n",
      "2017-07-20 20:24:47,251 : INFO : resetting layer weights\n",
      "2017-07-20 20:24:47,736 : INFO : training model with 7 workers on 47134 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2017-07-20 20:24:48,740 : INFO : PROGRESS: at 1.62% examples, 990959 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:24:49,749 : INFO : PROGRESS: at 3.35% examples, 1021463 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:24:50,763 : INFO : PROGRESS: at 4.82% examples, 980923 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-20 20:24:51,770 : INFO : PROGRESS: at 6.29% examples, 962777 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:24:52,772 : INFO : PROGRESS: at 7.87% examples, 965824 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:24:53,783 : INFO : PROGRESS: at 9.49% examples, 970483 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:24:54,792 : INFO : PROGRESS: at 11.18% examples, 980482 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:24:55,792 : INFO : PROGRESS: at 12.84% examples, 985782 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:24:56,803 : INFO : PROGRESS: at 14.61% examples, 997186 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:24:57,813 : INFO : PROGRESS: at 16.40% examples, 1004667 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:24:58,823 : INFO : PROGRESS: at 18.06% examples, 1005578 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:24:59,831 : INFO : PROGRESS: at 19.79% examples, 1009032 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:25:00,849 : INFO : PROGRESS: at 21.32% examples, 1001957 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:01,851 : INFO : PROGRESS: at 22.97% examples, 1002855 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:25:02,856 : INFO : PROGRESS: at 24.71% examples, 1007497 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:03,862 : INFO : PROGRESS: at 26.50% examples, 1013736 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:04,869 : INFO : PROGRESS: at 28.25% examples, 1017566 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:25:05,876 : INFO : PROGRESS: at 30.02% examples, 1021322 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:25:06,879 : INFO : PROGRESS: at 31.76% examples, 1024093 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:07,880 : INFO : PROGRESS: at 33.52% examples, 1027334 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:25:08,887 : INFO : PROGRESS: at 35.29% examples, 1029501 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:09,893 : INFO : PROGRESS: at 37.03% examples, 1030815 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:10,897 : INFO : PROGRESS: at 38.75% examples, 1031916 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 20:25:11,902 : INFO : PROGRESS: at 40.53% examples, 1034261 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:25:12,905 : INFO : PROGRESS: at 42.29% examples, 1035685 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:25:13,906 : INFO : PROGRESS: at 44.10% examples, 1038722 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:14,934 : INFO : PROGRESS: at 45.87% examples, 1039900 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 20:25:15,951 : INFO : PROGRESS: at 47.45% examples, 1037511 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 20:25:16,955 : INFO : PROGRESS: at 49.21% examples, 1039011 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:17,962 : INFO : PROGRESS: at 50.91% examples, 1039335 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:25:18,965 : INFO : PROGRESS: at 52.63% examples, 1039949 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:19,972 : INFO : PROGRESS: at 54.26% examples, 1038858 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:25:20,980 : INFO : PROGRESS: at 55.98% examples, 1038698 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:21,986 : INFO : PROGRESS: at 57.72% examples, 1039460 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:22,989 : INFO : PROGRESS: at 59.39% examples, 1038981 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:24,010 : INFO : PROGRESS: at 61.03% examples, 1037591 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:25,015 : INFO : PROGRESS: at 62.60% examples, 1035172 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:25:26,015 : INFO : PROGRESS: at 64.23% examples, 1034456 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:25:27,033 : INFO : PROGRESS: at 65.62% examples, 1029661 words/s, in_qsize 10, out_qsize 3\n",
      "2017-07-20 20:25:28,048 : INFO : PROGRESS: at 67.18% examples, 1027940 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:29,056 : INFO : PROGRESS: at 68.76% examples, 1026464 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:30,070 : INFO : PROGRESS: at 70.32% examples, 1024737 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:25:31,076 : INFO : PROGRESS: at 71.97% examples, 1024486 words/s, in_qsize 10, out_qsize 1\n",
      "2017-07-20 20:25:32,077 : INFO : PROGRESS: at 73.62% examples, 1024320 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-20 20:25:33,094 : INFO : PROGRESS: at 75.34% examples, 1024646 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:34,112 : INFO : PROGRESS: at 77.07% examples, 1025010 words/s, in_qsize 10, out_qsize 4\n",
      "2017-07-20 20:25:35,115 : INFO : PROGRESS: at 78.89% examples, 1026964 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:36,120 : INFO : PROGRESS: at 80.66% examples, 1028054 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 20:25:37,134 : INFO : PROGRESS: at 82.49% examples, 1029566 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:38,137 : INFO : PROGRESS: at 84.29% examples, 1031099 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:25:39,145 : INFO : PROGRESS: at 86.02% examples, 1031894 words/s, in_qsize 8, out_qsize 4\n",
      "2017-07-20 20:25:40,150 : INFO : PROGRESS: at 87.82% examples, 1033435 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:41,157 : INFO : PROGRESS: at 89.61% examples, 1034681 words/s, in_qsize 12, out_qsize 0\n",
      "2017-07-20 20:25:42,165 : INFO : PROGRESS: at 91.37% examples, 1035601 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:25:43,180 : INFO : PROGRESS: at 93.17% examples, 1036694 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:44,184 : INFO : PROGRESS: at 94.97% examples, 1038003 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:45,187 : INFO : PROGRESS: at 96.75% examples, 1038748 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:46,191 : INFO : PROGRESS: at 98.55% examples, 1039847 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:25:46,968 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-07-20 20:25:46,970 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-07-20 20:25:46,971 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-07-20 20:25:46,972 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-20 20:25:46,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-20 20:25:46,976 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-20 20:25:46,978 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-20 20:25:46,979 : INFO : training on 85026035 raw words (61668516 effective words) took 59.2s, 1040969 effective words/s\n",
      "2017-07-20 20:25:46,980 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing the memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:25:47,294 : INFO : saving Word2Vec object under /home/snu/data/Test_data/models/2017-07-20_20-24-40/output_0, separately None\n",
      "2017-07-20 20:25:47,294 : INFO : not storing attribute syn0norm\n",
      "2017-07-20 20:25:47,295 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:25:47,593 : INFO : saved /home/snu/data/Test_data/models/2017-07-20_20-24-40/output_0\n",
      "2017-07-20 20:25:47,680 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS353-english-rel.txt: 0.5095\n",
      "2017-07-20 20:25:47,680 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS353-english-rel.txt: 0.5090\n",
      "2017-07-20 20:25:47,681 : INFO : Pairs with unknown words ratio: 0.4%\n",
      "2017-07-20 20:25:47,732 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS-353-EN.txt: 0.5568\n",
      "2017-07-20 20:25:47,732 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS-353-EN.txt: 0.5861\n",
      "2017-07-20 20:25:47,733 : INFO : Pairs with unknown words ratio: 6.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:25:47,825 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS353-english-sim.txt: 0.6409\n",
      "2017-07-20 20:25:47,825 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS353-english-sim.txt: 0.6327\n",
      "2017-07-20 20:25:47,826 : INFO : Pairs with unknown words ratio: 1.0%\n",
      "2017-07-20 20:25:47,883 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/SE17-EN.txt: 0.6235\n",
      "2017-07-20 20:25:47,884 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/SE17-EN.txt: 0.6102\n",
      "2017-07-20 20:25:47,885 : INFO : Pairs with unknown words ratio: 32.6%\n",
      "2017-07-20 20:25:47,977 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WordSimilarity-353-EN.txt: 0.6561\n",
      "2017-07-20 20:25:47,978 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WordSimilarity-353-EN.txt: 0.6777\n",
      "2017-07-20 20:25:47,979 : INFO : Pairs with unknown words ratio: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test results: \n",
      "\n",
      "Average Pearson Correlation  = 0.60\n",
      "Average Pearson Spearman rank-order correlation = 0.60\n",
      "Total number of missing words : 179/1389\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "from os import walk\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "def train_and_test(root_train, root_test , output_name, params):\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    directory = os.path.join(\"/home/snu/data/Test_data/models\", datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    os.makedirs(directory)\n",
    "    test=open(directory+\"/test results.txt\",\"w\") ##replaced the 'directory' into real one\n",
    "    startTime = time.time()\n",
    "      \n",
    "    print(\"\\nRetrieving the corpus...\")   \n",
    "    #if using another corpus then use LineSentence() which will itterate over the corpus in root \n",
    "    #sentences = LineSentence(root)\n",
    "    sentences = Text8Corpus(root_train)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model = Word2Vec(sentences, **params)\n",
    "    \n",
    "    print(\"Freeing the memory...\")\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    print(\"saving the model...\")\n",
    "    model.save(directory+\"/\"+output_name)\n",
    "    \n",
    "    print(\"Testing the model...\")\n",
    "    test.write(\"Model \" + output_name +  \" at \" + directory + \".\\n\")\n",
    "    test.write(\"Training from : \" + root_train + \"\\n\")\n",
    "    test.write(\"\\n\")\n",
    "    endTime = time.time()\n",
    "    test.write(\"Parameters : \" + \"\\n\\tVector size = \" + repr(params[\"size\"]) + \",\\n\\tWindow size = \" + repr(params[\"window\"]) + \",\\n\\tMin count = \" + repr(params[\"min_count\"]) + \",\\n\\tskip-gram/CBOW = \" + (\"skip-gram\" if params[\"sg\"]==1 else \"CBOW\") + \",\\n\\tHierarchical softmax/Negative sampling = \" + (\"Hierarchical softmax\" if params[\"hs\"]==1 else \"Negative sampling \\n\\n\"))\n",
    "    test.write(\"The model took \" + repr((endTime - startTime)/60)+ \" to train.\" + \"\\n\")\n",
    "    test.write(\"Vocabulary length : \" + repr(len(model.wv.vocab)) + \"\\n\")\n",
    "    test.write(\"\\n\\n\") \n",
    "    test.write(\"Testing from : \" + root_test + \"\\n\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "    for (dirpath, dirnames, filenames) in walk(root_test):\n",
    "        filenames = filenames\n",
    "        break\n",
    "    \n",
    "    sim=0   \n",
    "    sim2=0\n",
    "    num_tests=len(filenames)   \n",
    "    mw=0\n",
    "    total_num_pairs=0\n",
    "    \n",
    "    for file in filenames:\n",
    "        similarity = model.wv.evaluate_word_pairs(root_test+file, dummy4unknown=False)\n",
    "        num_pairs=round(len(open(root_test+file,\"r\").readlines()))\n",
    "        total_num_pairs=total_num_pairs + num_pairs\n",
    "        sim=sim+similarity[0][0]\n",
    "        sim2=sim2+similarity[1][0]\n",
    "        mw=mw+similarity[2]*num_pairs/100\n",
    "        test.write(\"Test results on \" + file + \": \\n\")\n",
    "        test.write(\"Pearson correlation coefficient = %.2f\\n\" % similarity[0][0])\n",
    "        test.write(\"Spearman rank-order correlation coefficient = %.2f\\n\" % similarity[1][0])\n",
    "        test.write(\"Number of missing words = \" + repr(round(similarity[2]*num_pairs/100)) + \"/\" + repr(num_pairs)+ \"\\n\")\n",
    "        test.write(\"\\n\")\n",
    "    \n",
    "    test.write(\"Average test results: \\n\")    \n",
    "    test.write(\"Average Pearson Correlation  = %.2f\\n\" % (sim/num_tests))\n",
    "    test.write(\"Average Pearson Spearman rank-order correlation = %.2f\\n\" % (sim2/num_tests))\n",
    "    test.write(\"Total number of missing words : \"+repr(round(mw))+\"/\"+repr(total_num_pairs)+ \"\\n\")\n",
    "\n",
    "    test.close()\n",
    "    \n",
    "    return (sim/num_tests), (sim2/num_tests), (round(mw)), (total_num_pairs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    output_name = \"output_0\"\n",
    "    root_train = \"/home/snu/data/Training_data/text8\"\n",
    "    root_test = \"/home/snu/data/Test_data/\"\n",
    "    \n",
    "    params = {\n",
    "        'size': 100,\n",
    "        'window': 8,\n",
    "        'min_count': 10,\n",
    "        'sg' : 0,\n",
    "        'hs' : 0,\n",
    "        'workers': max(1, multiprocessing.cpu_count() - 1),\n",
    "        'sample': 1E-3,\n",
    "        }\n",
    "    \n",
    "    results = train_and_test(root_train, root_test, output_name, params)\n",
    "    \n",
    "    print(\"\\nAverage test results: \\n\")    \n",
    "    print(\"Average Pearson Correlation  = %.2f\" % results[0])\n",
    "    print(\"Average Pearson Spearman rank-order correlation = %.2f\" % results[1])\n",
    "    print(\"Total number of missing words : \" + repr(results[2])+\"/\" + repr(results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
