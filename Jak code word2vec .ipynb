{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-23 16:26:34,145 : INFO : collecting all words and their counts\n",
      "2017-07-23 16:26:34,147 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving the corpus...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-23 16:26:39,595 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-07-23 16:26:39,596 : INFO : Loading a fresh vocabulary\n",
      "2017-07-23 16:26:39,769 : INFO : min_count=10 retains 47134 unique words (18% of original 253854, drops 206720)\n",
      "2017-07-23 16:26:39,770 : INFO : min_count=10 leaves 16561031 word corpus (97% of original 17005207, drops 444176)\n",
      "2017-07-23 16:26:39,870 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-07-23 16:26:39,875 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-07-23 16:26:39,876 : INFO : downsampling leaves estimated 12333563 word corpus (74.5% of prior 16561031)\n",
      "2017-07-23 16:26:39,877 : INFO : estimated required memory for 47134 words and 100 dimensions: 61274200 bytes\n",
      "2017-07-23 16:26:40,025 : INFO : resetting layer weights\n",
      "2017-07-23 16:26:40,453 : INFO : training model with 7 workers on 47134 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-07-23 16:26:41,484 : INFO : PROGRESS: at 0.85% examples, 509723 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:42,502 : INFO : PROGRESS: at 1.69% examples, 506348 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:43,503 : INFO : PROGRESS: at 2.52% examples, 505353 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:44,509 : INFO : PROGRESS: at 3.33% examples, 503381 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:45,515 : INFO : PROGRESS: at 4.12% examples, 499344 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:46,531 : INFO : PROGRESS: at 4.97% examples, 503610 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:47,547 : INFO : PROGRESS: at 5.80% examples, 504094 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:48,564 : INFO : PROGRESS: at 6.68% examples, 508799 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:26:49,570 : INFO : PROGRESS: at 7.58% examples, 514483 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:50,571 : INFO : PROGRESS: at 8.48% examples, 518178 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:51,573 : INFO : PROGRESS: at 9.34% examples, 519355 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:52,580 : INFO : PROGRESS: at 10.16% examples, 518282 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:53,591 : INFO : PROGRESS: at 10.97% examples, 516575 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:54,640 : INFO : PROGRESS: at 11.82% examples, 515340 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:55,651 : INFO : PROGRESS: at 12.62% examples, 513599 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:56,679 : INFO : PROGRESS: at 13.44% examples, 512403 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:57,690 : INFO : PROGRESS: at 14.30% examples, 513299 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:58,716 : INFO : PROGRESS: at 15.17% examples, 513341 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:26:59,723 : INFO : PROGRESS: at 15.92% examples, 510193 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:00,732 : INFO : PROGRESS: at 16.61% examples, 505740 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:01,734 : INFO : PROGRESS: at 17.45% examples, 506137 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:02,740 : INFO : PROGRESS: at 18.28% examples, 506353 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:03,774 : INFO : PROGRESS: at 19.20% examples, 508086 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:04,779 : INFO : PROGRESS: at 20.12% examples, 510151 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:05,786 : INFO : PROGRESS: at 21.01% examples, 511587 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:27:06,788 : INFO : PROGRESS: at 21.96% examples, 514027 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:07,801 : INFO : PROGRESS: at 22.90% examples, 516096 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:08,803 : INFO : PROGRESS: at 23.72% examples, 515548 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:09,808 : INFO : PROGRESS: at 24.53% examples, 514975 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:10,818 : INFO : PROGRESS: at 25.42% examples, 516203 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:11,827 : INFO : PROGRESS: at 26.28% examples, 516703 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:12,832 : INFO : PROGRESS: at 27.08% examples, 516079 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:13,847 : INFO : PROGRESS: at 27.98% examples, 517174 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:14,861 : INFO : PROGRESS: at 28.92% examples, 518881 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:15,896 : INFO : PROGRESS: at 29.86% examples, 520170 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-23 16:27:16,941 : INFO : PROGRESS: at 30.75% examples, 520207 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:17,948 : INFO : PROGRESS: at 31.56% examples, 519671 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:27:18,957 : INFO : PROGRESS: at 32.33% examples, 518528 words/s, in_qsize 12, out_qsize 0\n",
      "2017-07-23 16:27:19,958 : INFO : PROGRESS: at 33.20% examples, 518968 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:20,964 : INFO : PROGRESS: at 34.04% examples, 518848 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:21,965 : INFO : PROGRESS: at 34.92% examples, 519521 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:22,985 : INFO : PROGRESS: at 35.83% examples, 519772 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:23,994 : INFO : PROGRESS: at 36.66% examples, 519500 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:24,994 : INFO : PROGRESS: at 37.40% examples, 518071 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:26,022 : INFO : PROGRESS: at 38.30% examples, 518472 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:27,027 : INFO : PROGRESS: at 39.15% examples, 518625 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:28,032 : INFO : PROGRESS: at 39.93% examples, 517647 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:29,043 : INFO : PROGRESS: at 40.80% examples, 517889 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:30,059 : INFO : PROGRESS: at 41.70% examples, 518330 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:27:31,068 : INFO : PROGRESS: at 42.63% examples, 519261 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:32,097 : INFO : PROGRESS: at 43.50% examples, 519308 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:33,121 : INFO : PROGRESS: at 44.40% examples, 519683 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:34,131 : INFO : PROGRESS: at 45.14% examples, 518511 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:35,138 : INFO : PROGRESS: at 46.02% examples, 519051 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:36,147 : INFO : PROGRESS: at 46.91% examples, 519655 words/s, in_qsize 12, out_qsize 0\n",
      "2017-07-23 16:27:37,175 : INFO : PROGRESS: at 47.76% examples, 519483 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:27:38,188 : INFO : PROGRESS: at 48.63% examples, 519728 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:39,194 : INFO : PROGRESS: at 49.38% examples, 518757 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:40,205 : INFO : PROGRESS: at 50.23% examples, 518734 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:41,207 : INFO : PROGRESS: at 51.09% examples, 518948 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:42,211 : INFO : PROGRESS: at 51.96% examples, 519223 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:27:43,223 : INFO : PROGRESS: at 52.83% examples, 519418 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-23 16:27:44,239 : INFO : PROGRESS: at 53.72% examples, 519818 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:45,262 : INFO : PROGRESS: at 54.59% examples, 519929 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:46,290 : INFO : PROGRESS: at 55.47% examples, 519839 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:27:47,299 : INFO : PROGRESS: at 56.31% examples, 519649 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-23 16:27:48,322 : INFO : PROGRESS: at 57.12% examples, 519187 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:49,338 : INFO : PROGRESS: at 57.99% examples, 519343 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:50,340 : INFO : PROGRESS: at 58.85% examples, 519396 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:51,346 : INFO : PROGRESS: at 59.61% examples, 518615 words/s, in_qsize 13, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-23 16:27:52,367 : INFO : PROGRESS: at 60.48% examples, 518725 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:53,377 : INFO : PROGRESS: at 61.39% examples, 519057 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:54,389 : INFO : PROGRESS: at 62.28% examples, 519334 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-23 16:27:55,402 : INFO : PROGRESS: at 63.15% examples, 519470 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:56,418 : INFO : PROGRESS: at 64.03% examples, 519680 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:57,420 : INFO : PROGRESS: at 64.81% examples, 519216 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:27:58,450 : INFO : PROGRESS: at 65.63% examples, 518923 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-23 16:27:59,466 : INFO : PROGRESS: at 66.48% examples, 518941 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:28:00,467 : INFO : PROGRESS: at 67.29% examples, 518789 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:01,481 : INFO : PROGRESS: at 68.18% examples, 519095 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:02,499 : INFO : PROGRESS: at 69.12% examples, 519768 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:03,509 : INFO : PROGRESS: at 70.05% examples, 520369 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-23 16:28:04,515 : INFO : PROGRESS: at 70.96% examples, 520811 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:05,517 : INFO : PROGRESS: at 71.89% examples, 521443 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:28:06,519 : INFO : PROGRESS: at 72.79% examples, 521873 words/s, in_qsize 11, out_qsize 0\n",
      "2017-07-23 16:28:07,537 : INFO : PROGRESS: at 73.65% examples, 521868 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:08,545 : INFO : PROGRESS: at 74.52% examples, 522023 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:09,554 : INFO : PROGRESS: at 75.41% examples, 522139 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-23 16:28:10,581 : INFO : PROGRESS: at 76.34% examples, 522492 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:11,609 : INFO : PROGRESS: at 77.27% examples, 522878 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:28:12,627 : INFO : PROGRESS: at 78.22% examples, 523480 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:13,637 : INFO : PROGRESS: at 79.14% examples, 523842 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-23 16:28:14,652 : INFO : PROGRESS: at 80.09% examples, 524383 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-23 16:28:15,675 : INFO : PROGRESS: at 81.06% examples, 524976 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:16,694 : INFO : PROGRESS: at 82.02% examples, 525496 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:17,695 : INFO : PROGRESS: at 82.95% examples, 525947 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-23 16:28:18,713 : INFO : PROGRESS: at 83.89% examples, 526410 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:19,716 : INFO : PROGRESS: at 84.82% examples, 526915 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:20,731 : INFO : PROGRESS: at 85.74% examples, 527273 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:21,733 : INFO : PROGRESS: at 86.67% examples, 527787 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:22,734 : INFO : PROGRESS: at 87.60% examples, 528274 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:23,738 : INFO : PROGRESS: at 88.50% examples, 528556 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:24,767 : INFO : PROGRESS: at 89.44% examples, 528934 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:25,784 : INFO : PROGRESS: at 90.39% examples, 529400 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:26,785 : INFO : PROGRESS: at 91.31% examples, 529782 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:27,797 : INFO : PROGRESS: at 92.24% examples, 530143 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:28,835 : INFO : PROGRESS: at 93.18% examples, 530416 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:29,838 : INFO : PROGRESS: at 94.12% examples, 530881 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:30,866 : INFO : PROGRESS: at 95.06% examples, 531161 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-23 16:28:31,882 : INFO : PROGRESS: at 96.00% examples, 531416 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:32,882 : INFO : PROGRESS: at 96.93% examples, 531775 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:33,913 : INFO : PROGRESS: at 97.86% examples, 531993 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:34,915 : INFO : PROGRESS: at 98.80% examples, 532363 words/s, in_qsize 12, out_qsize 0\n",
      "2017-07-23 16:28:35,920 : INFO : PROGRESS: at 99.73% examples, 532659 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-23 16:28:36,139 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-07-23 16:28:36,147 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-07-23 16:28:36,158 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-07-23 16:28:36,163 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-23 16:28:36,169 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-23 16:28:36,174 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-23 16:28:36,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-23 16:28:36,187 : INFO : training on 85026035 raw words (61668885 effective words) took 115.7s, 532863 effective words/s\n",
      "2017-07-23 16:28:36,188 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing the memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-23 16:28:36,507 : INFO : saving Word2Vec object under /home/snu/data/Test_data/models/2017-07-23_16-26-34/output_0, separately None\n",
      "2017-07-23 16:28:36,508 : INFO : not storing attribute syn0norm\n",
      "2017-07-23 16:28:36,508 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-23 16:28:36,803 : INFO : saved /home/snu/data/Test_data/models/2017-07-23_16-26-34/output_0\n",
      "2017-07-23 16:28:36,885 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS353-english-rel.txt: 0.5481\n",
      "2017-07-23 16:28:36,886 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS353-english-rel.txt: 0.5444\n",
      "2017-07-23 16:28:36,886 : INFO : Pairs with unknown words ratio: 0.4%\n",
      "2017-07-23 16:28:36,936 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS-353-EN.txt: 0.6219\n",
      "2017-07-23 16:28:36,937 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS-353-EN.txt: 0.6550\n",
      "2017-07-23 16:28:36,937 : INFO : Pairs with unknown words ratio: 6.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-23 16:28:37,024 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS353-english-sim.txt: 0.6671\n",
      "2017-07-23 16:28:37,025 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS353-english-sim.txt: 0.6562\n",
      "2017-07-23 16:28:37,025 : INFO : Pairs with unknown words ratio: 1.0%\n",
      "2017-07-23 16:28:37,082 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/SE17-EN.txt: 0.6733\n",
      "2017-07-23 16:28:37,082 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/SE17-EN.txt: 0.6598\n",
      "2017-07-23 16:28:37,083 : INFO : Pairs with unknown words ratio: 32.6%\n",
      "2017-07-23 16:28:37,172 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WordSimilarity-353-EN.txt: 0.6783\n",
      "2017-07-23 16:28:37,173 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WordSimilarity-353-EN.txt: 0.6986\n",
      "2017-07-23 16:28:37,173 : INFO : Pairs with unknown words ratio: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test results: \n",
      "\n",
      "Average Pearson Correlation  = 0.64\n",
      "Average Pearson Spearman rank-order correlation = 0.64\n",
      "Total number of missing words : 179/1389\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "from os import walk\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "def train_and_test(root_train, root_test , output_name, params):\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    directory = os.path.join(\"/home/snu/data/Test_data/models\", datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    os.makedirs(directory)\n",
    "    test=open(directory+\"/test results.txt\",\"w\") ##replaced the 'directory' into real one\n",
    "    startTime = time.time()\n",
    "      \n",
    "    print(\"\\nRetrieving the corpus...\")   \n",
    "    #if using another corpus then use LineSentence() which will itterate over the corpus in root \n",
    "    #sentences = LineSentence(root)\n",
    "    sentences = Text8Corpus(root_train)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model = Word2Vec(sentences, **params)\n",
    "    \n",
    "    print(\"Freeing the memory...\")\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    print(\"saving the model...\")\n",
    "    model.save(directory+\"/\"+output_name)\n",
    "    \n",
    "    print(\"Testing the model...\")\n",
    "    test.write(\"Model \" + output_name +  \" at \" + directory + \".\\n\")\n",
    "    test.write(\"Training from : \" + root_train + \"\\n\")\n",
    "    test.write(\"\\n\")\n",
    "    endTime = time.time()\n",
    "    test.write(\"Parameters : \" + \"\\n\\tVector size = \" + repr(params[\"size\"]) + \",\\n\\tWindow size = \" + repr(params[\"window\"]) + \",\\n\\tMin count = \" + repr(params[\"min_count\"]) + \",\\n\\tskip-gram/CBOW = \" + (\"skip-gram\" if params[\"sg\"]==1 else \"CBOW\") + \",\\n\\tHierarchical softmax/Negative sampling = \" + (\"Hierarchical softmax\" if params[\"hs\"]==1 else \"Negative sampling \\n\\n\"))\n",
    "    test.write(\"The model took \" + repr((endTime - startTime)/60)+ \" to train.\" + \"\\n\")\n",
    "    test.write(\"Vocabulary length : \" + repr(len(model.wv.vocab)) + \"\\n\")\n",
    "    test.write(\"\\n\\n\") \n",
    "    test.write(\"Testing from : \" + root_test + \"\\n\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "    for (dirpath, dirnames, filenames) in walk(root_test):\n",
    "        filenames = filenames\n",
    "        break\n",
    "    \n",
    "    sim=0   \n",
    "    sim2=0\n",
    "    num_tests=len(filenames)   \n",
    "    mw=0\n",
    "    total_num_pairs=0\n",
    "    \n",
    "    for file in filenames:\n",
    "        similarity = model.wv.evaluate_word_pairs(root_test+file, dummy4unknown=False)\n",
    "        num_pairs=round(len(open(root_test+file,\"r\").readlines()))\n",
    "        total_num_pairs=total_num_pairs + num_pairs\n",
    "        sim=sim+similarity[0][0]\n",
    "        sim2=sim2+similarity[1][0]\n",
    "        mw=mw+similarity[2]*num_pairs/100\n",
    "        test.write(\"Test results on \" + file + \": \\n\")\n",
    "        test.write(\"Pearson correlation coefficient = %.2f\\n\" % similarity[0][0])\n",
    "        test.write(\"Spearman rank-order correlation coefficient = %.2f\\n\" % similarity[1][0])\n",
    "        test.write(\"Number of missing words = \" + repr(round(similarity[2]*num_pairs/100)) + \"/\" + repr(num_pairs)+ \"\\n\")\n",
    "        test.write(\"\\n\")\n",
    "    \n",
    "    test.write(\"Average test results: \\n\")    \n",
    "    test.write(\"Average Pearson Correlation  = %.2f\\n\" % (sim/num_tests))\n",
    "    test.write(\"Average Pearson Spearman rank-order correlation = %.2f\\n\" % (sim2/num_tests))\n",
    "    test.write(\"Total number of missing words : \"+repr(round(mw))+\"/\"+repr(total_num_pairs)+ \"\\n\")\n",
    "\n",
    "    test.close()\n",
    "    \n",
    "    return (sim/num_tests), (sim2/num_tests), (round(mw)), (total_num_pairs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    output_name = \"output_0\"\n",
    "    root_train = \"/home/snu/data/Training_data/text8\"\n",
    "    root_test = \"/home/snu/data/Test_data/\"\n",
    "    \n",
    "    params = {\n",
    "        'size': 100,\n",
    "        'window': 5,\n",
    "        'min_count': 10,\n",
    "        'sg' : 1,\n",
    "        'hs' : 0,\n",
    "        'workers': max(1, multiprocessing.cpu_count() - 1),\n",
    "        'sample': 1E-3,\n",
    "        }\n",
    "    \n",
    "    results = train_and_test(root_train, root_test, output_name, params)\n",
    "    \n",
    "    print(\"\\nAverage test results: \\n\")    \n",
    "    print(\"Average Pearson Correlation  = %.2f\" % results[0])\n",
    "    print(\"Average Pearson Spearman rank-order correlation = %.2f\" % results[1])\n",
    "    print(\"Total number of missing words : \" + repr(results[2])+\"/\" + repr(results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
