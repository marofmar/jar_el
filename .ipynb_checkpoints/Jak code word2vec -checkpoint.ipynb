{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 09:53:21,229 : INFO : collecting all words and their counts\n",
      "2017-07-20 09:53:21,230 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving the corpus...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 09:53:26,806 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-07-20 09:53:26,807 : INFO : Loading a fresh vocabulary\n",
      "2017-07-20 09:53:26,994 : INFO : min_count=10 retains 47134 unique words (18% of original 253854, drops 206720)\n",
      "2017-07-20 09:53:26,995 : INFO : min_count=10 leaves 16561031 word corpus (97% of original 17005207, drops 444176)\n",
      "2017-07-20 09:53:27,108 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-07-20 09:53:27,114 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-07-20 09:53:27,115 : INFO : downsampling leaves estimated 12333563 word corpus (74.5% of prior 16561031)\n",
      "2017-07-20 09:53:27,116 : INFO : estimated required memory for 47134 words and 100 dimensions: 61274200 bytes\n",
      "2017-07-20 09:53:27,285 : INFO : resetting layer weights\n",
      "2017-07-20 09:53:27,739 : INFO : training model with 7 workers on 47134 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2017-07-20 09:53:28,742 : INFO : PROGRESS: at 1.70% examples, 1042040 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:29,749 : INFO : PROGRESS: at 3.32% examples, 1011768 words/s, in_qsize 9, out_qsize 3\n",
      "2017-07-20 09:53:30,757 : INFO : PROGRESS: at 5.06% examples, 1031130 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:53:31,759 : INFO : PROGRESS: at 6.73% examples, 1033563 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 09:53:32,770 : INFO : PROGRESS: at 8.44% examples, 1037471 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:53:33,771 : INFO : PROGRESS: at 10.16% examples, 1041973 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:34,780 : INFO : PROGRESS: at 11.85% examples, 1041220 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 09:53:35,781 : INFO : PROGRESS: at 13.57% examples, 1043645 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:53:36,790 : INFO : PROGRESS: at 15.27% examples, 1042730 words/s, in_qsize 13, out_qsize 3\n",
      "2017-07-20 09:53:37,795 : INFO : PROGRESS: at 16.97% examples, 1041510 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:38,806 : INFO : PROGRESS: at 18.69% examples, 1042282 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:53:39,819 : INFO : PROGRESS: at 20.45% examples, 1044122 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:53:40,826 : INFO : PROGRESS: at 22.19% examples, 1044715 words/s, in_qsize 11, out_qsize 1\n",
      "2017-07-20 09:53:41,830 : INFO : PROGRESS: at 23.86% examples, 1043366 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:53:42,831 : INFO : PROGRESS: at 25.56% examples, 1044465 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:53:43,849 : INFO : PROGRESS: at 27.30% examples, 1045905 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 09:53:44,858 : INFO : PROGRESS: at 28.99% examples, 1045416 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 09:53:45,873 : INFO : PROGRESS: at 30.73% examples, 1046263 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:46,878 : INFO : PROGRESS: at 32.49% examples, 1048004 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:47,886 : INFO : PROGRESS: at 34.17% examples, 1047281 words/s, in_qsize 9, out_qsize 2\n",
      "2017-07-20 09:53:48,892 : INFO : PROGRESS: at 35.91% examples, 1047438 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:49,903 : INFO : PROGRESS: at 37.58% examples, 1045927 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:50,908 : INFO : PROGRESS: at 39.26% examples, 1045282 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:51,921 : INFO : PROGRESS: at 41.01% examples, 1045929 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 09:53:52,921 : INFO : PROGRESS: at 42.73% examples, 1045991 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:53:53,931 : INFO : PROGRESS: at 44.40% examples, 1044966 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 09:53:54,957 : INFO : PROGRESS: at 46.07% examples, 1043893 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:53:55,961 : INFO : PROGRESS: at 47.77% examples, 1044294 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:53:56,971 : INFO : PROGRESS: at 49.43% examples, 1043375 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 09:53:57,977 : INFO : PROGRESS: at 51.15% examples, 1043821 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:53:58,980 : INFO : PROGRESS: at 52.87% examples, 1044512 words/s, in_qsize 12, out_qsize 0\n",
      "2017-07-20 09:54:00,002 : INFO : PROGRESS: at 54.58% examples, 1044156 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:54:01,017 : INFO : PROGRESS: at 56.32% examples, 1043975 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:02,020 : INFO : PROGRESS: at 58.01% examples, 1043948 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:03,024 : INFO : PROGRESS: at 59.72% examples, 1043753 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 09:54:04,029 : INFO : PROGRESS: at 61.48% examples, 1044577 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:54:05,040 : INFO : PROGRESS: at 63.20% examples, 1044531 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 09:54:06,054 : INFO : PROGRESS: at 64.93% examples, 1044818 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 09:54:07,060 : INFO : PROGRESS: at 66.62% examples, 1044986 words/s, in_qsize 10, out_qsize 3\n",
      "2017-07-20 09:54:08,075 : INFO : PROGRESS: at 68.34% examples, 1045092 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 09:54:09,088 : INFO : PROGRESS: at 70.04% examples, 1045039 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:10,092 : INFO : PROGRESS: at 71.63% examples, 1043474 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-20 09:54:11,097 : INFO : PROGRESS: at 73.30% examples, 1043085 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 09:54:12,099 : INFO : PROGRESS: at 74.94% examples, 1042499 words/s, in_qsize 12, out_qsize 0\n",
      "2017-07-20 09:54:13,103 : INFO : PROGRESS: at 76.64% examples, 1042010 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:14,106 : INFO : PROGRESS: at 78.37% examples, 1042410 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:15,121 : INFO : PROGRESS: at 80.05% examples, 1041839 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 09:54:16,134 : INFO : PROGRESS: at 81.80% examples, 1042159 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:17,145 : INFO : PROGRESS: at 83.50% examples, 1042066 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:54:18,151 : INFO : PROGRESS: at 85.21% examples, 1042244 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:19,154 : INFO : PROGRESS: at 86.85% examples, 1041922 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 09:54:20,169 : INFO : PROGRESS: at 88.35% examples, 1039386 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 09:54:21,179 : INFO : PROGRESS: at 89.94% examples, 1038150 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:22,191 : INFO : PROGRESS: at 91.64% examples, 1038237 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 09:54:23,197 : INFO : PROGRESS: at 93.33% examples, 1038289 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:24,201 : INFO : PROGRESS: at 95.01% examples, 1038208 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:54:25,212 : INFO : PROGRESS: at 96.73% examples, 1038101 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 09:54:26,222 : INFO : PROGRESS: at 98.46% examples, 1038346 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 09:54:27,085 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-07-20 09:54:27,097 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-07-20 09:54:27,103 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-07-20 09:54:27,106 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-20 09:54:27,108 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-20 09:54:27,109 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-20 09:54:27,114 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-20 09:54:27,115 : INFO : training on 85026035 raw words (61666385 effective words) took 59.4s, 1038611 effective words/s\n",
      "2017-07-20 09:54:27,116 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing the memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 09:54:27,438 : INFO : saving Word2Vec object under /home/snu/data/Test_data/models/2017-07-20_09-53-21/output_0, separately None\n",
      "2017-07-20 09:54:27,439 : INFO : not storing attribute syn0norm\n",
      "2017-07-20 09:54:27,439 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 09:54:27,756 : INFO : saved /home/snu/data/Test_data/models/2017-07-20_09-53-21/output_0\n",
      "2017-07-20 09:54:27,849 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS353-english-rel.txt: 0.5111\n",
      "2017-07-20 09:54:27,849 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS353-english-rel.txt: 0.5122\n",
      "2017-07-20 09:54:27,850 : INFO : Pairs with unknown words ratio: 0.4%\n",
      "2017-07-20 09:54:27,909 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS-353-EN.txt: 0.5812\n",
      "2017-07-20 09:54:27,910 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS-353-EN.txt: 0.6265\n",
      "2017-07-20 09:54:27,910 : INFO : Pairs with unknown words ratio: 6.2%\n",
      "2017-07-20 09:54:28,005 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WS353-english-sim.txt: 0.6370\n",
      "2017-07-20 09:54:28,006 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WS353-english-sim.txt: 0.6258\n",
      "2017-07-20 09:54:28,007 : INFO : Pairs with unknown words ratio: 1.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 09:54:28,076 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/SE17-EN.txt: 0.6170\n",
      "2017-07-20 09:54:28,077 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/SE17-EN.txt: 0.6067\n",
      "2017-07-20 09:54:28,077 : INFO : Pairs with unknown words ratio: 32.6%\n",
      "2017-07-20 09:54:28,175 : INFO : Pearson correlation coefficient against /home/snu/data/Test_data/WordSimilarity-353-EN.txt: 0.6589\n",
      "2017-07-20 09:54:28,176 : INFO : Spearman rank-order correlation coefficient against /home/snu/data/Test_data/WordSimilarity-353-EN.txt: 0.6799\n",
      "2017-07-20 09:54:28,177 : INFO : Pairs with unknown words ratio: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test results: \n",
      "\n",
      "Average Pearson Correlation  = 0.60\n",
      "Average Pearson Spearman rank-order correlation = 0.61\n",
      "Total number of missing words : 179/1389\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "from os import walk\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "def train_and_test(root_train, root_test , output_name, params):\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    directory = os.path.join(\"/home/snu/data/Test_data/models\", datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    os.makedirs(directory)\n",
    "    test=open(directory+\"/test results.txt\",\"w\") ##replaced the 'directory' into real one\n",
    "    startTime = time.time()\n",
    "      \n",
    "    print(\"\\nRetrieving the corpus...\")   \n",
    "    #if using another corpus then use LineSentence() which will itterate over the corpus in root \n",
    "    #sentences = LineSentence(root)\n",
    "    sentences = Text8Corpus(root_train)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model = Word2Vec(sentences, **params)\n",
    "    \n",
    "    print(\"Freeing the memory...\")\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    print(\"saving the model...\")\n",
    "    model.save(directory+\"/\"+output_name)\n",
    "    \n",
    "    print(\"Testing the model...\")\n",
    "    test.write(\"Model \" + output_name +  \" at \" + directory + \".\\n\")\n",
    "    test.write(\"Training from : \" + root_train + \"\\n\")\n",
    "    test.write(\"\\n\")\n",
    "    endTime = time.time()\n",
    "    test.write(\"Parameters : \" + \"\\n\\tVector size = \" + repr(params[\"size\"]) + \",\\n\\tWindow size = \" + repr(params[\"window\"]) + \",\\n\\tMin count = \" + repr(params[\"min_count\"]) + \",\\n\\tskip-gram/CBOW = \" + (\"skip-gram\" if params[\"sg\"]==1 else \"CBOW\") + \",\\n\\tHierarchical softmax/Negative sampling = \" + (\"Hierarchical softmax\" if params[\"hs\"]==1 else \"Negative sampling \\n\\n\"))\n",
    "    test.write(\"The model took \" + repr((endTime - startTime)/60)+ \" to train.\" + \"\\n\")\n",
    "    test.write(\"Vocabulary length : \" + repr(len(model.wv.vocab)) + \"\\n\")\n",
    "    test.write(\"\\n\\n\") \n",
    "    test.write(\"Testing from : \" + root_test + \"\\n\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "    for (dirpath, dirnames, filenames) in walk(root_test):\n",
    "        filenames = filenames\n",
    "        break\n",
    "    \n",
    "    sim=0   \n",
    "    sim2=0\n",
    "    num_tests=len(filenames)   \n",
    "    mw=0\n",
    "    total_num_pairs=0\n",
    "    \n",
    "    for file in filenames:\n",
    "        similarity = model.wv.evaluate_word_pairs(root_test+file, dummy4unknown=False)\n",
    "        num_pairs=round(len(open(root_test+file,\"r\").readlines()))\n",
    "        total_num_pairs=total_num_pairs + num_pairs\n",
    "        sim=sim+similarity[0][0]\n",
    "        sim2=sim2+similarity[1][0]\n",
    "        mw=mw+similarity[2]*num_pairs/100\n",
    "        test.write(\"Test results on \" + file + \": \\n\")\n",
    "        test.write(\"Pearson correlation coefficient = %.2f\\n\" % similarity[0][0])\n",
    "        test.write(\"Spearman rank-order correlation coefficient = %.2f\\n\" % similarity[1][0])\n",
    "        test.write(\"Number of missing words = \" + repr(round(similarity[2]*num_pairs/100)) + \"/\" + repr(num_pairs)+ \"\\n\")\n",
    "        test.write(\"\\n\")\n",
    "    \n",
    "    test.write(\"Average test results: \\n\")    \n",
    "    test.write(\"Average Pearson Correlation  = %.2f\\n\" % (sim/num_tests))\n",
    "    test.write(\"Average Pearson Spearman rank-order correlation = %.2f\\n\" % (sim2/num_tests))\n",
    "    test.write(\"Total number of missing words : \"+repr(round(mw))+\"/\"+repr(total_num_pairs)+ \"\\n\")\n",
    "\n",
    "    test.close()\n",
    "    \n",
    "    return (sim/num_tests), (sim2/num_tests), (round(mw)), (total_num_pairs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    output_name = \"output_0\"\n",
    "    root_train = \"/home/snu/data/Training_data/text8\"\n",
    "    root_test = \"/home/snu/data/Test_data/\"\n",
    "    \n",
    "    params = {\n",
    "        'size': 100,\n",
    "        'window': 8,\n",
    "        'min_count': 10,\n",
    "        'sg' : 0,\n",
    "        'hs' : 0,\n",
    "        'workers': max(1, multiprocessing.cpu_count() - 1),\n",
    "        'sample': 1E-3,\n",
    "        }\n",
    "    \n",
    "    results = train_and_test(root_train, root_test, output_name, params)\n",
    "    \n",
    "    print(\"\\nAverage test results: \\n\")    \n",
    "    print(\"Average Pearson Correlation  = %.2f\" % results[0])\n",
    "    print(\"Average Pearson Spearman rank-order correlation = %.2f\" % results[1])\n",
    "    print(\"Total number of missing words : \" + repr(results[2])+\"/\" + repr(results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
