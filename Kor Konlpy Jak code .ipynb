{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-14 19:34:53,920 : INFO : collecting all words and their counts\n",
      "2017-07-14 19:34:53,922 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving the corpus...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-14 19:34:59,815 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-07-14 19:34:59,816 : INFO : Loading a fresh vocabulary\n",
      "2017-07-14 19:35:00,018 : INFO : min_count=10 retains 47134 unique words (18% of original 253854, drops 206720)\n",
      "2017-07-14 19:35:00,019 : INFO : min_count=10 leaves 16561031 word corpus (97% of original 17005207, drops 444176)\n",
      "2017-07-14 19:35:00,146 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-07-14 19:35:00,153 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-07-14 19:35:00,154 : INFO : downsampling leaves estimated 12333563 word corpus (74.5% of prior 16561031)\n",
      "2017-07-14 19:35:00,155 : INFO : estimated required memory for 47134 words and 100 dimensions: 61274200 bytes\n",
      "2017-07-14 19:35:00,327 : INFO : resetting layer weights\n",
      "2017-07-14 19:35:00,866 : INFO : training model with 7 workers on 47134 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-07-14 19:35:01,869 : INFO : PROGRESS: at 1.60% examples, 978790 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:02,875 : INFO : PROGRESS: at 3.26% examples, 995067 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:03,885 : INFO : PROGRESS: at 4.93% examples, 1004545 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:04,892 : INFO : PROGRESS: at 6.60% examples, 1012324 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:05,903 : INFO : PROGRESS: at 8.28% examples, 1015933 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-14 19:35:06,913 : INFO : PROGRESS: at 9.92% examples, 1014947 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-14 19:35:07,926 : INFO : PROGRESS: at 11.60% examples, 1016848 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:08,942 : INFO : PROGRESS: at 13.30% examples, 1018477 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-14 19:35:09,945 : INFO : PROGRESS: at 14.94% examples, 1018464 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:10,946 : INFO : PROGRESS: at 16.63% examples, 1018139 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:11,953 : INFO : PROGRESS: at 18.27% examples, 1017111 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:12,961 : INFO : PROGRESS: at 19.93% examples, 1016514 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-14 19:35:13,965 : INFO : PROGRESS: at 21.65% examples, 1018541 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:14,979 : INFO : PROGRESS: at 23.35% examples, 1019578 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:15,980 : INFO : PROGRESS: at 24.94% examples, 1017068 words/s, in_qsize 10, out_qsize 3\n",
      "2017-07-14 19:35:16,998 : INFO : PROGRESS: at 26.49% examples, 1012977 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:18,004 : INFO : PROGRESS: at 28.18% examples, 1014825 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:19,014 : INFO : PROGRESS: at 29.86% examples, 1015800 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:20,016 : INFO : PROGRESS: at 31.51% examples, 1015857 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:21,022 : INFO : PROGRESS: at 33.16% examples, 1015688 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:22,033 : INFO : PROGRESS: at 34.80% examples, 1015395 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:23,048 : INFO : PROGRESS: at 36.44% examples, 1013439 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-14 19:35:24,058 : INFO : PROGRESS: at 38.12% examples, 1014101 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-14 19:35:25,064 : INFO : PROGRESS: at 39.74% examples, 1012912 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:26,073 : INFO : PROGRESS: at 41.40% examples, 1012619 words/s, in_qsize 13, out_qsize 2\n",
      "2017-07-14 19:35:27,074 : INFO : PROGRESS: at 42.97% examples, 1010807 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:28,083 : INFO : PROGRESS: at 44.50% examples, 1007989 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-14 19:35:29,102 : INFO : PROGRESS: at 46.15% examples, 1008077 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-14 19:35:30,104 : INFO : PROGRESS: at 47.82% examples, 1009010 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-14 19:35:31,117 : INFO : PROGRESS: at 49.47% examples, 1008942 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-14 19:35:32,137 : INFO : PROGRESS: at 51.15% examples, 1009347 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-14 19:35:33,139 : INFO : PROGRESS: at 52.80% examples, 1009736 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:34,144 : INFO : PROGRESS: at 54.46% examples, 1010101 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:35,147 : INFO : PROGRESS: at 56.12% examples, 1009880 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-14 19:35:36,155 : INFO : PROGRESS: at 57.78% examples, 1009953 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-14 19:35:37,162 : INFO : PROGRESS: at 59.41% examples, 1009545 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-14 19:35:38,167 : INFO : PROGRESS: at 61.07% examples, 1009654 words/s, in_qsize 13, out_qsize 2\n",
      "2017-07-14 19:35:39,169 : INFO : PROGRESS: at 62.74% examples, 1009857 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-14 19:35:40,173 : INFO : PROGRESS: at 64.42% examples, 1010418 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:41,190 : INFO : PROGRESS: at 66.09% examples, 1010803 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-14 19:35:42,201 : INFO : PROGRESS: at 67.67% examples, 1009785 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:43,212 : INFO : PROGRESS: at 69.17% examples, 1007682 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:44,223 : INFO : PROGRESS: at 70.65% examples, 1005328 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:45,225 : INFO : PROGRESS: at 72.30% examples, 1005628 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:46,233 : INFO : PROGRESS: at 73.96% examples, 1005882 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:47,235 : INFO : PROGRESS: at 75.59% examples, 1005580 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-14 19:35:48,252 : INFO : PROGRESS: at 77.28% examples, 1005994 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-14 19:35:49,253 : INFO : PROGRESS: at 78.95% examples, 1006365 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:35:50,263 : INFO : PROGRESS: at 80.58% examples, 1006047 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:51,268 : INFO : PROGRESS: at 82.09% examples, 1004267 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-14 19:35:52,272 : INFO : PROGRESS: at 83.75% examples, 1004504 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-14 19:35:53,273 : INFO : PROGRESS: at 85.41% examples, 1004973 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-14 19:35:54,277 : INFO : PROGRESS: at 86.96% examples, 1004251 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:55,283 : INFO : PROGRESS: at 88.57% examples, 1004015 words/s, in_qsize 12, out_qsize 3\n",
      "2017-07-14 19:35:56,290 : INFO : PROGRESS: at 90.26% examples, 1004671 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-14 19:35:57,298 : INFO : PROGRESS: at 91.91% examples, 1004805 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-14 19:35:58,308 : INFO : PROGRESS: at 93.57% examples, 1004971 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:35:59,314 : INFO : PROGRESS: at 95.11% examples, 1003871 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:36:00,321 : INFO : PROGRESS: at 96.72% examples, 1003385 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-14 19:36:01,331 : INFO : PROGRESS: at 98.40% examples, 1003728 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-14 19:36:02,251 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-07-14 19:36:02,253 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-07-14 19:36:02,255 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-07-14 19:36:02,265 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-14 19:36:02,267 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-14 19:36:02,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-14 19:36:02,282 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-14 19:36:02,284 : INFO : training on 85026035 raw words (61667532 effective words) took 61.4s, 1004107 effective words/s\n",
      "2017-07-14 19:36:02,286 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing the memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-14 19:36:02,626 : INFO : saving Word2Vec object under /home/snu/data/Test_data/models/2017-07-14_19-34-53/output_0, separately None\n",
      "2017-07-14 19:36:02,627 : INFO : not storing attribute syn0norm\n",
      "2017-07-14 19:36:02,628 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-14 19:36:02,949 : INFO : saved /home/snu/data/Test_data/models/2017-07-14_19-34-53/output_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filenames' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-96e56e76d3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         }\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAverage test results: \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-96e56e76d3da>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(root_train, root_test, output_name, params)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0msim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mnum_tests\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m## should these block be indented??? or moved before the for loop right above?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mmw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mtotal_num_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'filenames' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "from os import walk\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "def train_and_test(root_train, root_test , output_name, params):\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    directory = os.path.join(\"/home/snu/data/Test_data/models\", datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    os.makedirs(directory)\n",
    "    test=open(directory+\"/test results.txt\",\"w\") ##replaced the 'directory' into real one\n",
    "    startTime = time.time()\n",
    "      \n",
    "    print(\"\\nRetrieving the corpus...\")   \n",
    "    #if using another corpus then use LineSentence() which will itterate over the corpus in root \n",
    "    #sentences = LineSentence(root)\n",
    "    sentences = Text8Corpus(root_train)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model = Word2Vec(sentences, **params)\n",
    "    \n",
    "    print(\"Freeing the memory...\")\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    print(\"saving the model...\")\n",
    "    model.save(directory+\"/\"+output_name)\n",
    "    \n",
    "    print(\"Testing the model...\")\n",
    "    test.write(\"Model \" + output_name +  \" at \" + directory + \".\\n\")\n",
    "    test.write(\"Training from : \" + root_train + \"\\n\")\n",
    "    test.write(\"\\n\")\n",
    "    endTime = time.time()\n",
    "    test.write(\"Parameters : \" + \"\\n\\tVector size = \" + repr(params[\"size\"]) + \",\\n\\tWindow size = \" + repr(params[\"window\"]) + \",\\n\\tMin count = \" + repr(params[\"min_count\"]) + \",\\n\\tskip-gram/CBOW = \" + (\"skip-gram\" if params[\"sg\"]==1 else \"CBOW\") + \",\\n\\tHierarchical softmax/Negative sampling = \" + (\"Hierarchical softmax\" if params[\"hs\"]==1 else \"Negative sampling \\n\\n\"))\n",
    "    test.write(\"The model took \" + repr((endTime - startTime)/60)+ \" to train.\" + \"\\n\")\n",
    "    test.write(\"Vocabulary length : \" + repr(len(model.wv.vocab)) + \"\\n\")\n",
    "    test.write(\"\\n\\n\") \n",
    "    test.write(\"Testing from : \" + root_test + \"\\n\\n\")\n",
    "    \n",
    "    for (dirpath, dirnames, filenames) in walk(root_test):\n",
    "        filenames = filenames\n",
    "        break\n",
    "    \n",
    "    sim=0   \n",
    "    sim2=0           # local variable filenames referenced before assignment\n",
    "    num_tests=len(filenames)   # should these block be indented??? or moved before the for loop right above?\n",
    "    mw=0\n",
    "    total_num_pairs=0\n",
    "    \n",
    "    for file in filenames:\n",
    "        similarity = model.wv.evaluate_word_pairs(root_test+file, dummy4unknown=False)\n",
    "        num_pairs=round(len(open(root_test+file,\"r\").readlines()))\n",
    "        total_num_pairs=total_num_pairs + num_pairs\n",
    "        sim=sim+similarity[0][0]\n",
    "        sim2=sim2+similarity[1][0]\n",
    "        mw=mw+similarity[2]*num_pairs/100\n",
    "        test.write(\"Test results on \" + file + \": \\n\")\n",
    "        test.write(\"Pearson correlation coefficient = %.2f\\n\" % similarity[0][0])\n",
    "        test.write(\"Spearman rank-order correlation coefficient = %.2f\\n\" % similarity[1][0])\n",
    "        test.write(\"Number of missing words = \" + repr(round(similarity[2]*num_pairs/100)) + \"/\" + repr(num_pairs)+ \"\\n\")\n",
    "        test.write(\"\\n\")\n",
    "    \n",
    "    test.write(\"Average test results: \\n\")    \n",
    "    test.write(\"Average Pearson Correlation  = %.2f\\n\" % (sim/num_tests))\n",
    "    test.write(\"Average Pearson Spearman rank-order correlation = %.2f\\n\" % (sim2/num_tests))\n",
    "    test.write(\"Total number of missing words : \"+repr(round(mw))+\"/\"+repr(total_num_pairs)+ \"\\n\")\n",
    "\n",
    "    test.close()\n",
    "    \n",
    "    return (sim/num_tests), (sim2/num_tests), (round(mw)), (total_num_pairs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    output_name = \"output_0\"\n",
    "    root_train = \"/home/snu/data/Training_data/text8\"\n",
    "    root_test = \"/home/snu/data/Test_data/SE17-EN.txt\"\n",
    "    \n",
    "    params = {\n",
    "        'size': 100,\n",
    "        'window': 5,\n",
    "        'min_count': 10,\n",
    "        'sg' : 0,\n",
    "        'hs' : 0,\n",
    "        'workers': max(1, multiprocessing.cpu_count() - 1),\n",
    "        'sample': 1E-3,\n",
    "        }\n",
    "    \n",
    "    results = train_and_test(root_train, root_test, output_name, params)\n",
    "    \n",
    "    print(\"\\nAverage test results: \\n\")    \n",
    "    print(\"Average Pearson Correlation  = %.2f\" % results[0])\n",
    "    print(\"Average Pearson Spearman rank-order correlation = %.2f\" % results[1])\n",
    "    print(\"Total number of missing words : \" + repr(results[2])+\"/\" + repr(results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
