{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:17:56,682 : INFO : collecting all words and their counts\n",
      "2017-07-20 20:17:56,684 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving the corpus...\n",
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:18:02,365 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2017-07-20 20:18:02,366 : INFO : Loading a fresh vocabulary\n",
      "2017-07-20 20:18:02,567 : INFO : min_count=10 retains 47134 unique words (18% of original 253854, drops 206720)\n",
      "2017-07-20 20:18:02,568 : INFO : min_count=10 leaves 16561031 word corpus (97% of original 17005207, drops 444176)\n",
      "2017-07-20 20:18:02,689 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2017-07-20 20:18:02,696 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2017-07-20 20:18:02,697 : INFO : downsampling leaves estimated 12333563 word corpus (74.5% of prior 16561031)\n",
      "2017-07-20 20:18:02,698 : INFO : estimated required memory for 47134 words and 100 dimensions: 61274200 bytes\n",
      "2017-07-20 20:18:02,862 : INFO : resetting layer weights\n",
      "2017-07-20 20:18:03,305 : INFO : training model with 7 workers on 47134 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=8\n",
      "2017-07-20 20:18:04,321 : INFO : PROGRESS: at 1.50% examples, 907220 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:18:05,333 : INFO : PROGRESS: at 3.22% examples, 974596 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:06,346 : INFO : PROGRESS: at 4.99% examples, 1008808 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:07,351 : INFO : PROGRESS: at 6.57% examples, 1003752 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 20:18:08,363 : INFO : PROGRESS: at 8.31% examples, 1015898 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:09,378 : INFO : PROGRESS: at 9.78% examples, 996206 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:18:10,389 : INFO : PROGRESS: at 11.55% examples, 1008235 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:11,390 : INFO : PROGRESS: at 13.16% examples, 1006626 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:12,390 : INFO : PROGRESS: at 14.72% examples, 1002653 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-20 20:18:13,397 : INFO : PROGRESS: at 16.34% examples, 999856 words/s, in_qsize 10, out_qsize 3\n",
      "2017-07-20 20:18:14,399 : INFO : PROGRESS: at 18.07% examples, 1005798 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:15,405 : INFO : PROGRESS: at 19.75% examples, 1007025 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:18:16,415 : INFO : PROGRESS: at 21.52% examples, 1011726 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:17,429 : INFO : PROGRESS: at 23.15% examples, 1010114 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:18,436 : INFO : PROGRESS: at 24.83% examples, 1011809 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:19,440 : INFO : PROGRESS: at 26.51% examples, 1013828 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:20,447 : INFO : PROGRESS: at 28.16% examples, 1013788 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 20:18:21,453 : INFO : PROGRESS: at 29.75% examples, 1011813 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:18:22,471 : INFO : PROGRESS: at 31.42% examples, 1012028 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:23,476 : INFO : PROGRESS: at 33.02% examples, 1010559 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:24,484 : INFO : PROGRESS: at 34.47% examples, 1005156 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:25,499 : INFO : PROGRESS: at 36.00% examples, 1000921 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:26,516 : INFO : PROGRESS: at 37.45% examples, 995385 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:27,522 : INFO : PROGRESS: at 38.95% examples, 992194 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:28,524 : INFO : PROGRESS: at 40.72% examples, 995804 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:29,531 : INFO : PROGRESS: at 42.52% examples, 999335 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:30,555 : INFO : PROGRESS: at 44.26% examples, 1001182 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:18:31,555 : INFO : PROGRESS: at 45.98% examples, 1003894 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 20:18:32,564 : INFO : PROGRESS: at 47.48% examples, 1001069 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:33,579 : INFO : PROGRESS: at 48.87% examples, 995849 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:34,581 : INFO : PROGRESS: at 50.26% examples, 991660 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:35,589 : INFO : PROGRESS: at 51.65% examples, 987310 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:36,595 : INFO : PROGRESS: at 52.98% examples, 982150 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:37,603 : INFO : PROGRESS: at 54.37% examples, 978337 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:18:38,632 : INFO : PROGRESS: at 55.97% examples, 977322 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:18:39,639 : INFO : PROGRESS: at 57.61% examples, 978078 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:40,657 : INFO : PROGRESS: at 59.22% examples, 977949 words/s, in_qsize 13, out_qsize 2\n",
      "2017-07-20 20:18:41,671 : INFO : PROGRESS: at 60.93% examples, 979389 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:18:42,696 : INFO : PROGRESS: at 61.99% examples, 970194 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:43,725 : INFO : PROGRESS: at 63.03% examples, 961395 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:44,754 : INFO : PROGRESS: at 64.03% examples, 952389 words/s, in_qsize 12, out_qsize 2\n",
      "2017-07-20 20:18:45,792 : INFO : PROGRESS: at 65.01% examples, 943398 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 20:18:46,819 : INFO : PROGRESS: at 66.03% examples, 935873 words/s, in_qsize 14, out_qsize 2\n",
      "2017-07-20 20:18:47,828 : INFO : PROGRESS: at 67.01% examples, 928321 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:48,835 : INFO : PROGRESS: at 68.02% examples, 921528 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:49,851 : INFO : PROGRESS: at 68.92% examples, 913479 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:50,855 : INFO : PROGRESS: at 69.90% examples, 906915 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:51,862 : INFO : PROGRESS: at 70.85% examples, 900245 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:52,870 : INFO : PROGRESS: at 71.80% examples, 893810 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:53,872 : INFO : PROGRESS: at 72.82% examples, 888487 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:18:54,877 : INFO : PROGRESS: at 73.80% examples, 883017 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:55,880 : INFO : PROGRESS: at 74.78% examples, 877678 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:56,882 : INFO : PROGRESS: at 75.79% examples, 872567 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:18:57,889 : INFO : PROGRESS: at 76.75% examples, 867338 words/s, in_qsize 12, out_qsize 3\n",
      "2017-07-20 20:18:58,896 : INFO : PROGRESS: at 77.81% examples, 863358 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:18:59,924 : INFO : PROGRESS: at 78.85% examples, 858869 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:00,926 : INFO : PROGRESS: at 79.76% examples, 853684 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:19:01,946 : INFO : PROGRESS: at 80.79% examples, 849603 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:19:02,966 : INFO : PROGRESS: at 81.72% examples, 844488 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:03,978 : INFO : PROGRESS: at 82.72% examples, 840532 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:04,985 : INFO : PROGRESS: at 83.73% examples, 836904 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:05,989 : INFO : PROGRESS: at 84.69% examples, 833049 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:19:06,996 : INFO : PROGRESS: at 85.71% examples, 829885 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 20:19:08,008 : INFO : PROGRESS: at 86.65% examples, 825977 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:09,027 : INFO : PROGRESS: at 87.63% examples, 822412 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:19:10,042 : INFO : PROGRESS: at 88.65% examples, 819393 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:11,054 : INFO : PROGRESS: at 89.64% examples, 816173 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:12,061 : INFO : PROGRESS: at 90.68% examples, 813516 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:13,067 : INFO : PROGRESS: at 91.69% examples, 810768 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:14,068 : INFO : PROGRESS: at 92.67% examples, 807955 words/s, in_qsize 13, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:19:15,078 : INFO : PROGRESS: at 93.69% examples, 805281 words/s, in_qsize 14, out_qsize 0\n",
      "2017-07-20 20:19:16,078 : INFO : PROGRESS: at 94.69% examples, 802735 words/s, in_qsize 11, out_qsize 2\n",
      "2017-07-20 20:19:17,096 : INFO : PROGRESS: at 95.74% examples, 800270 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:18,101 : INFO : PROGRESS: at 96.72% examples, 797537 words/s, in_qsize 14, out_qsize 1\n",
      "2017-07-20 20:19:19,115 : INFO : PROGRESS: at 97.71% examples, 794887 words/s, in_qsize 13, out_qsize 1\n",
      "2017-07-20 20:19:20,120 : INFO : PROGRESS: at 98.71% examples, 792484 words/s, in_qsize 13, out_qsize 0\n",
      "2017-07-20 20:19:21,137 : INFO : PROGRESS: at 99.62% examples, 789337 words/s, in_qsize 12, out_qsize 1\n",
      "2017-07-20 20:19:21,469 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2017-07-20 20:19:21,487 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2017-07-20 20:19:21,492 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2017-07-20 20:19:21,499 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-07-20 20:19:21,503 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-07-20 20:19:21,510 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-07-20 20:19:21,512 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-07-20 20:19:21,513 : INFO : training on 85026035 raw words (61664664 effective words) took 78.2s, 788492 effective words/s\n",
      "2017-07-20 20:19:21,514 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeing the memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:19:21,973 : INFO : saving Word2Vec object under /home/snu/data/Test_data/models/2017-07-20_20-17-56/output_0, separately None\n",
      "2017-07-20 20:19:21,974 : INFO : not storing attribute syn0norm\n",
      "2017-07-20 20:19:21,975 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-07-20 20:19:22,445 : INFO : saved /home/snu/data/Test_data/models/2017-07-20_20-17-56/output_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filenames' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a90fa19832b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         }\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAverage test results: \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a90fa19832b8>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m(root_train, root_test, output_name, params)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0msim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0msim2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mnum_tests\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mmw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtotal_num_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'filenames' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "from os import walk\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "def train_and_test(root_train, root_test , output_name, params):\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    directory = os.path.join(\"/home/snu/data/Test_data/models\", datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "    os.makedirs(directory)\n",
    "    test=open(directory+\"/test results.txt\",\"w\") ##replaced the 'directory' into real one\n",
    "    startTime = time.time()\n",
    "      \n",
    "    print(\"\\nRetrieving the corpus...\")   \n",
    "    #if using another corpus then use LineSentence() which will itterate over the corpus in root \n",
    "    #sentences = LineSentence(root)\n",
    "    sentences = Text8Corpus(root_train)\n",
    "    \n",
    "    print(\"Training the model...\")\n",
    "    model = Word2Vec(sentences, **params)\n",
    "    \n",
    "    print(\"Freeing the memory...\")\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    print(\"saving the model...\")\n",
    "    model.save(directory+\"/\"+output_name)\n",
    "    \n",
    "    print(\"Testing the model...\")\n",
    "    test.write(\"Model \" + output_name +  \" at \" + directory + \".\\n\")\n",
    "    test.write(\"Training from : \" + root_train + \"\\n\")\n",
    "    test.write(\"\\n\")\n",
    "    endTime = time.time()\n",
    "    test.write(\"Parameters : \" + \"\\n\\tVector size = \" + repr(params[\"size\"]) + \",\\n\\tWindow size = \" + repr(params[\"window\"]) + \",\\n\\tMin count = \" + repr(params[\"min_count\"]) + \",\\n\\tskip-gram/CBOW = \" + (\"skip-gram\" if params[\"sg\"]==1 else \"CBOW\") + \",\\n\\tHierarchical softmax/Negative sampling = \" + (\"Hierarchical softmax\" if params[\"hs\"]==1 else \"Negative sampling \\n\\n\"))\n",
    "    test.write(\"The model took \" + repr((endTime - startTime)/60)+ \" to train.\" + \"\\n\")\n",
    "    test.write(\"Vocabulary length : \" + repr(len(model.wv.vocab)) + \"\\n\")\n",
    "    test.write(\"\\n\\n\") \n",
    "    test.write(\"Testing from : \" + root_test + \"\\n\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "    for (dirpath, dirnames, filenames) in walk(root_test):\n",
    "        filenames = filenames\n",
    "        break\n",
    "    \n",
    "    sim=0   \n",
    "    sim2=0\n",
    "    num_tests=len(filenames)   \n",
    "    mw=0\n",
    "    total_num_pairs=0\n",
    "    \n",
    "    for file in filenames:\n",
    "        similarity = model.wv.evaluate_word_pairs(root_test+file, dummy4unknown=False)\n",
    "        num_pairs=round(len(open(root_test+file,\"r\").readlines()))\n",
    "        total_num_pairs=total_num_pairs + num_pairs\n",
    "        sim=sim+similarity[0][0]\n",
    "        sim2=sim2+similarity[1][0]\n",
    "        mw=mw+similarity[2]*num_pairs/100\n",
    "        test.write(\"Test results on \" + file + \": \\n\")\n",
    "        test.write(\"Pearson correlation coefficient = %.2f\\n\" % similarity[0][0])\n",
    "        test.write(\"Spearman rank-order correlation coefficient = %.2f\\n\" % similarity[1][0])\n",
    "        test.write(\"Number of missing words = \" + repr(round(similarity[2]*num_pairs/100)) + \"/\" + repr(num_pairs)+ \"\\n\")\n",
    "        test.write(\"\\n\")\n",
    "    \n",
    "    test.write(\"Average test results: \\n\")    \n",
    "    test.write(\"Average Pearson Correlation  = %.2f\\n\" % (sim/num_tests))\n",
    "    test.write(\"Average Pearson Spearman rank-order correlation = %.2f\\n\" % (sim2/num_tests))\n",
    "    test.write(\"Total number of missing words : \"+repr(round(mw))+\"/\"+repr(total_num_pairs)+ \"\\n\")\n",
    "\n",
    "    test.close()\n",
    "    \n",
    "    return (sim/num_tests), (sim2/num_tests), (round(mw)), (total_num_pairs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    output_name = \"output_0\"\n",
    "    root_train = \"/home/snu/data/Training_data/text8\"\n",
    "    root_test = \"//home/snu/data/Kor_Test_data/kor_ws353.txt\"\n",
    "    \n",
    "    params = {\n",
    "        'size': 100,\n",
    "        'window': 8,\n",
    "        'min_count': 10,\n",
    "        'sg' : 0,\n",
    "        'hs' : 0,\n",
    "        'workers': max(1, multiprocessing.cpu_count() - 1),\n",
    "        'sample': 1E-3,\n",
    "        }\n",
    "    \n",
    "    results = train_and_test(root_train, root_test, output_name, params)\n",
    "    \n",
    "    print(\"\\nAverage test results: \\n\")    \n",
    "    print(\"Average Pearson Correlation  = %.2f\" % results[0])\n",
    "    print(\"Average Pearson Spearman rank-order correlation = %.2f\" % results[1])\n",
    "    print(\"Total number of missing words : \" + repr(results[2])+\"/\" + repr(results[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
